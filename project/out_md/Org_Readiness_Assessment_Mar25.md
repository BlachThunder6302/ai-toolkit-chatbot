# Org_Readiness_Assessment_Mar25

## Page 1

Organizational Readiness Assessment Questionnaire REVISED FEBRUARY 2024

Images on this page:

\[image\]: media\Org_Readiness_Assessment_Mar25\page-1-img-1.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-1-img-2.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-1-img-3.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-1-img-4.png

## Page 2

\[No extractable text on this page\]

Images on this page:

\[image\]: media\Org_Readiness_Assessment_Mar25\page-2-img-1.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-2-img-2.png

## Page 3

Organizational Readiness Assessment Questionnaire DISCLAIMER The contents of this document are for information purposes only. INTERPOL and UNICRI assume no liability or responsibility for any inaccurate or incomplete information, nor for any actions taken in reliance thereon. The published material is distributed without warranty of any kind, either express or implied, and the responsibility for the interpretation and use of the material lies with the reader. In no event shall INTERPOL or UNICRI be liable for damages arising from its use. INTERPOL and UNICRI take no responsibility for the content of any external website referenced in this publication or for any defamatory, offensive or misleading information which might be contained on these third-party websites. Any links to external websites do not constitute an endorsement by INTERPOL or UNICRI, and are only provided as a convenience. It is the responsibility of the reader to evaluate the content and usefulness of information obtained from other sites. The views, thoughts and opinions expressed in the content of this publication belong solely to the authors and do not necessarily reflect the views or policies of, nor do they imply any endorsement by, INTERPOL or the United Nations, their member countries or member states, their governing bodies, or contributory organizations. Therefore, INTERPOL and UNICRI carry no responsibility for the opinions expressed in this publication. INTERPOL and UNICRI do not endorse or recommend any product, process, or service. Therefore, mention of any products, processes, or services in this document cannot be construed as an endorsement or recommendation by INTERPOL or UNICRI. The designation employed and presentation of the material in this document do not imply the expression of any opinion whatsoever on the part of the Secretariat of the United Nations, UNICRI or INTERPOL, concerning the legal status of any country, territory, city or area of its authorities, or concerning the delimitation of its frontiers or boundaries. The contents of this document may be quoted or reproduced, provided that the source of information is acknowledged. INTERPOL and UNICRI would like to receive a copy of the document in which this publication is used or quoted. 1

## Page 4

Organizational Readiness Assessment Questionnaire OVERVIEW WHAT The Organizational Readiness Assessment Questionnaire is a selfassessment questionnaire designed to support law enforcement agencies in determining how well equipped they are to adopt responsible AI in their agency based on a critical review of their culture, people, expertise, and the processes they have in place. The assessment will allow agencies to gain a better understanding of where they are in the journey toward responsible AI. The Organizational Readiness Assessment Questionnaire is closely linked to the Organizational Roadmap and the Principles for Responsible AI Innovation, and seeks to support agencies to implement the recommendations they contain. WHEN The Organizational Readiness Assessment Questionnaire is designed as a starting point for any agency wishing to explore responsible AI. It may also be useful for periodic internal reviews or to track progress towards full implementation of the Principles for Responsible AI Innovation. WHO This assessment tool is intended for use by law enforcement agency personnel engaged in the strategic side of the work the agency. This would specifically entail the chief of police and executive leadership, as well as decision-makers in senior management positions outside of the executive leadership, particularly those in technology and innovation units that are responsible for the use of AI systems. Some sub-sections of this assessment will require the designated respondent to coordinate with and collect input from other departments or units such as legal, procurement, or human resources.

## Page 5

Table of Contents DISCLAIMER 1 OVERVIEW 2 Levels of organizational readiness 4 Assessing your organizational readiness 6 CULTURE 6 PEOPLE AND EXPERTISE 9 PROCESSES 13 SCORECARD 16 RECOMMENDATIONS FOR ORGANIZATIONAL READINESS 17

## Page 6

Organizational Readiness Assessment Questionnaire Levels of organizational readiness Implementing responsible AI innovation at an institutional level in law enforcement agency requires certain processes, a conducive culture, and the right people and expertise to effectively harness its potential and minimize any possible negative impact on the people that officers are sworn to serve and protect. \|► Learn more about the organizational components for responsible AI innovation in the Organizational Roadmap. This self-assessment questionnaire will help you to identify where your agency stands regarding responsible AI innovation readiness and provide insight into the steps required to move the process forward, as well as other useful recommendations for developing and maintaining responsible AI innovation in your agency. For this self-assessment, five levels of responsible AI innovation readiness have been identified and will be used to communicate the results. The various levels of readiness reflect the fact that law enforcement agencies across the world are at various stages in their digital transformation journey and, by extension, their responsible AI innovation maturity journey. A brief description of each level of organizational readiness is provided below. LEVELS OF RESPONSIBLE AI INNOVATION READINESS LEVEL 1: This stage of responsible AI innovation maturity typically sees agencies with little or no experience with or awareness of AI systems, and they may or may not have some fundamental awareness of the potential ethical and 1 human rights challenges. Generally, agencies at this level will either not be using AI systems or will be beginning to consider using AI systems, although in some cases they may already be using AI systems to support their work or improve delivery. LEVEL 2: This stage of responsible AI innovation maturity typically sees agencies actively exploring AI systems and their responsible use. Agencies at this level will be 2 taking steps to conceptualize specific ethical and human rights guidance on selected AI use cases in a controlled environment (for instance, sandbox) before rolling them out. This stage also includes engagement with the public and multidisciplinary experts. 4

## Page 7

Organizational Readiness Assessment Questionnaire LEVEL 3: This stage of responsible AI innovation maturity typically sees agencies seeking to formalize processes for responsible AI innovation. Agencies at this level are in 3 the process of rolling out AI systems following successful public engagement and positive results from a pilot phase. Agencies will have more structured ethical and legal procedures around the daily use of AI systems to support and ensure the responsible use of these systems. LEVEL 4: This stage of responsible AI innovation maturity typically sees agencies moving to more creative ways to use AI systems and, in turn, adjusting their responsible AI innovation practices to address new challenges around 4 development, stakeholder management, evaluation, and monitoring. Agencies at this level will have taken a significant leap in their use of AI systems, from simple tasks to more complex tasks such as surveillance and facial recognition. LEVEL 5: This stage of responsible AI innovation maturity typically sees a more widespread use of AI systems, with agencies incorporating them into many of their core 5 functions, activities, duties, roles, and processes. Agencies at this level will have developed robust responsible AI innovation practices and a sustainable culture across their various units and departments. 5

## Page 8

Organizational Readiness Assessment Questionnaire Assessing your organizational readiness This organizational readiness assessment is framed around three components which are considered essential for responsible AI innovation in an agency, namely organizational culture, processes for responsible AI innovation –including governance frameworks –, and people and expertise. These components will be explored over the course of the following subsections in order to determine your agency’s readiness level. Each subsection contains several statements to which the respondent should assign a score from to 4, reflecting the extent to which they believe the statements to be true for their agency. For scoring purposes, equates to ‘not true’ or ‘don’t know’ and 4 equates to ‘very true’. At the end of each subsection, the respondent should tally individual scores to calculate the total number of points. Respondents should complete all subsections of the assessment fully to ensure as complete and accurate a result as possible. Finally, it is recommended that the assessment should be completed at least twice by different representatives to avoid subjective feedback influencing the overall outcome. The more conservative of the results should be considered as the final result. CULTURE This section of the assessment is focused on the culture within your agency, in particular, around responsible AI innovation, and is intended to help assess how well your agency understands and facilitates responsible AI innovation practices. This section will require input from your agency’s human resources and public relations team. \|► Learn more about fostering a responsible AI innovation culture in the Organizational Roadmap. Assign a score between and 4 to 1 2 3 4 reflect the extent to which you believe Untrue / Somewhat True / Usually Very true / the statements below to be true for Never true / untrue / true / true Always true Don't know Rarely true Sometimes your agency true 1. We understand that responsible AI innovation is a long journey that does □ □ □ □ □ not necessarily reach an end-point where everything is ‘finished’. 6

## Page 9

Organizational Readiness Assessment Questionnaire 2. We understand that the use of AI systems may not always be the right □ □ □ □ □ approach and that our journey should start with a critical self-assessment of the need for a particular system. 3. We engage with all relevant stakeholders to bring in the needed □ □ □ □ □ expertise and benefit from different perspectives. 4. We are aware of the value of organizing consultation sessions with experts from academia, civil □ □ □ □ □ society groups and the public to explain our AI innovation goals and how we would use AI systems responsibly. 5. We are familiar with the risks of AI □ □ □ □ □ systems in general, as well as the specific risks of our use case. 6. We understand that our use of AI systems may have negative □ □ □ □ □ consequences for individuals and the wider public. 7. We understand that AI systems should be developed, procured and used in accordance with national and regional laws or policies, as well as applicable international laws, in □ □ □ □ □ particular those relating to human rights, and that there should always be a clearly established legitimate law enforcement objective for their use. 8. We incentivize the responsible use of AI systems within our agency and the □ □ □ □ □ adoption of responsible AI innovation initiatives. 7

## Page 10

Organizational Readiness Assessment Questionnaire 9. We are aware that there may be a need for a new institutional architecture to facilitate our responsible AI innovation journey, which may involve the onboarding □ □ □ □ □ of new expertise, (re-) assignment of personnel to new tasks, as well as establishing partnerships with external stakeholders, particularly with industry, academia, and civil society. 10. We have developed new policies, such as a responsible AI strategy and standard operating procedures, □ □ □ □ □ and have established new structures, such as a responsible AI innovation oversight committee. 11. We implement our responsible AI □ □ □ □ □ strategy across all departments and units. 12. We invest financially and otherwise □ □ □ □ □ in facilitating and improving our responsible AI innovation initiatives. 13. We invest in providing training for the □ □ □ □ □ end-users of our AI systems. 14. We invest in the design, development, acquisition, and use of □ □ □ □ □ AI systems, as well as their long-term maintenance and upkeep. 15. We are prepared for the fact that implementing AI systems and any □ □ □ □ □ associated institutional architecture may be met with pushback. 16. We consider communication and transparency with the public □ □ □ □ □ regarding our use of AI systems to be priorities for our agency. 8

## Page 11

Organizational Readiness Assessment Questionnaire 17. We consider our use of specific AI systems to be open to public scrutiny □ □ □ □ □ and encourage public engagement and feedback in this area. 18. We are prepared to halt, recalibrate, or even decommission an AI system in the event that it is not of continued □ □ □ □ □ value, it is malfunctioning or causing harm, or if other circumstances that allowed for its initial use have changed. TOTAL SCORE: PEOPLE AND EXPERTISE This section of the assessment focuses on the individuals that play a key role in building and managing an agency’s AI system, as well as those involved in interpreting the system’s results. It is intended to help assess your agency’s readiness in terms of its technical and non-technical expertise and competencies. \|► Learn more about the necessary people and expertise in the Organizational Roadmap. 1 2 3 4 Assign a score between and 4 to reflect the extent to which you Untrue / Somewhat True / Usually Very true / believe the statements below to be Never true / untrue / true / true Always true for your agency Don't know Rarely true Sometimes true 1. The management and personnel in our agency involved in the design, development and use of AI systems □ □ □ □ □ have good understanding and knowledge of the Principles for Responsible AI Innovation. 2. The management and personnel involved in the design, development and use of AI systems follow the Principles for Responsible AI □ □ □ □ □ Innovation and identify and mitigate any legal and ethical concerns and negative consequences of the use of AI systems in line with the Principles. 9

## Page 12

Organizational Readiness Assessment Questionnaire 3. We have a responsible AI innovation oversight committee that works with the technology and innovation team, □ □ □ □ □ legal team, data protection officer and the communications team to implement responsible AI innovation in our agency. 4. All personnel interacting with or in charge of AI systems are aware of the AI systems’ capabilities and □ □ □ □ □ limitations and do not rely blindly and solely on outputs produced by these systems. 5. All personnel interacting with or in charge of AI systems receive training □ □ □ □ □ on the use of the system, which also incorporates a component on how to use them responsibly. 6. All personnel involved in the development and use of the AI system have a clear understanding of the role of the police in the □ □ □ □ □ criminal justice system, the underlying principles of policing and any associated codes of ethics, and the legality of using AI systems for evidence gathering. 7. All personnel involved in the development and use of the AI system have knowledge of national, regional and international □ □ □ □ □ laws, specifically including any requirements or limitations as they relate to the use of new technology in law enforcement. 8. We have an internal technology and innovation team capable of building AI systems in-house, or we outsource this function to a technology □ □ □ □ □ developer/provider and provide them with domain expertise and guidance on responsible AI innovation as required. 10

## Page 13

Organizational Readiness Assessment Questionnaire 9. We have experts with the necessary technical skills to integrate the AI □ □ □ □ □ system into our current infrastructure and procedures, or we outsource these functions. 10. The technical experts, internally or externally, who work on the development of AI systems □ □ □ □ □ understand and work towards our agency’s responsible AI strategy or roadmap for our identified use cases. 11. The technical experts, internally or externally, who work on the development of AI systems have knowledge of the technical tools or □ □ □ □ □ instruments, software, platforms, and guidance briefs that can support the responsible development and use of AI systems. 12. The technical experts, internally or externally, who work on the development of AI systems have access to law enforcement personnel with expertise in relevant areas □ □ □ □ □ of policing (narcotics, homicide, cybercrimes, human trafficking, biometrics etc.) to properly inform the development and implementation of our AI systems. 13. The technical experts, internally or externally, who work on the development of AI systems take into □ □ □ □ □ account ethical, legal and societal aspects and implement measures to prevent and mitigate adverse consequences. 14. The technical experts, internally or externally, who work on the development of AI systems □ □ □ □ □ understand the legal challenges AI systems may pose in terms of prosecution and how to safeguard admissibility. 11

## Page 14

Organizational Readiness Assessment Questionnaire 15. We have an individual or team that measures and evaluates the □ □ □ □ □ performance of our AI systems against set metrics. 16. We have a legal and data protection officer or team that ensures relevant data protection laws and □ □ □ □ □ requirements are considered during development and use and will work with relevant experts to ensure database security. 17. We have a cybersecurity expert or team that monitors, detects, investigates and responds to security □ □ □ □ □ threats, risks, and vulnerabilities related to our AI systems and related databases. 18. We have a communications and public relations officer or team that develops strategies and approaches □ □ □ □ □ for communicating relevant information about the (use of) AI systems to the public. 19. We have a communications and public relations officer or team that has received awareness-raising □ □ □ □ □ training on our use of AI systems and the associated risks to ensure the accuracy of communications with the public. 20. We engage with external expert groups including practitioners, academics, civil society groups, and community leaders to □ □ □ □ □ understand the national, regional, and sociocultural context that may affect the relationship between law enforcement and the public. 12

## Page 15

Organizational Readiness Assessment Questionnaire 21. We promote and prioritize the creation of diverse teams in terms of gender, race, culture etc. to ensure □ □ □ □ □ we have a well-rounded and broad perspective when developing, using or monitoring AI systems. TOTAL SCORE: PROCESSES This section of the assessment focuses specifically on how prepared a law enforcement agency is to implement the Principles for Responsible AI Innovation, become more accountable, and build a culture that supports a Responsible AI Strategy. It is intended to help assess your agency’s readiness based on its responsible AI initiatives. \|► Learn more about developing this strategy in the Organizational Roadmap. 1 2 3 4 Assign a score between and 4 to reflect the extent to which you Untrue / Somewhat True / Usually Very true / believe the statements below to be Never true / untrue / true / true Always true for your agency Don't know Rarely true Sometimes true 1. We have a clearly defined responsible AI strategy or comparable roadmap □ □ □ □ □ or vision for the responsible use of AI systems. 2. Our responsible AI strategy, roadmap □ □ □ □ □ or vision for the responsible use of AI systems is publicly accessible. 3. We have defined specific initiatives, processes and workflows to □ □ □ □ □ support the rollout and adoption of responsible AI innovation practices. 4. We have established a responsible AI innovation oversight committee or comparable advisory board made up of multidisciplinary □ □ □ □ □ experts (for instance, academia, civil society, human rights, ethics), that is responsible for oversight and accountability of our use of AI systems. 13

## Page 16

Organizational Readiness Assessment Questionnaire 5. We monitor and stay up to date with developments in best practices and □ □ □ □ □ national, regional, and international frameworks on the responsible use of AI systems. 6. We have undertaken an initial needs and capabilities assessment before □ □ □ □ □ beginning the process of developing or procuring AI systems. 7. We have a process for the selection and prioritization of the AI systems □ □ □ □ □ to be implemented, as well specific use cases, in line with our overall responsible AI strategy. 8. We conduct human rights impact assessments and data protection impact assessments, or have similar processes in place for identifying, □ □ □ □ □ preventing and mitigating any adverse effects on human rights associated with the use of an AI system. 9. We use the Risk Assessment Questionnaire, or a comparable instrument to identify any risks □ □ □ □ □ to individuals, society and the environment associated with the use of the AI system. 10. Before the development or procurement of a high-risk or controversial AI system, we require □ □ □ □ □ the relevant teams to engage with the public in coordination with the communications and public information officer or team. 11. Consultation with public/private advisory groups is obligatory before an AI system can be used, and we □ □ □ □ □ provide the opportunity for any issues regarding potential negative effects of the AI system and its use to be raised within our agency. 14

## Page 17

Organizational Readiness Assessment Questionnaire 12. We have a process for evaluating whether we have sufficient capabilities to develop an AI system internally or whether the system □ □ □ □ □ should be developed externally, developed jointly with external stakeholders, or procured ‘off-theshelf’. 13. We have a procurement process that considers system performance and □ □ □ □ □ the risk of error and bias, as well as auditing, testing and evaluating AI systems. 14. As part of our procurement process, we require potential technology developers/providers to organize an independent audit of their AI systems □ □ □ □ □ in order to check for unfairness or bias, interference with privacy and data protection, harm to individuals, or communities or groups, etc. 15. We implement audits/testing and □ □ □ □ □ model evaluation for our AI systems. 16. We require routine monitoring and evaluation of the performance of our AI system in order to ensure the use of the AI system continues to meet the overall law enforcement □ □ □ □ □ objective and that it is being used in accordance with the Principles for Responsible AI Innovation. This will trigger corrective action, maintenance, recalibration or decommissioning as needed. 17. We have clearly-defined metrics to appraise and monitor the continued □ □ □ □ □ success/performance of our AI systems in terms of operational outcomes and responsible use. 15

## Page 18

Organizational Readiness Assessment Questionnaire 18. We have processes for law enforcement personnel, independent auditors or evaluators, and the public □ □ □ □ □ to report or raise concerns about any issues with our AI systems or the use of these systems. TOTAL SCORE: SCORECARD Having completed the Organizational Readiness Assessment and assigned each statement a score between and 4, respondents can use this score card to interpret their results. The cumulative score of the sub-sections should be calculated by the respondent and compared with the corresponding sub-sections on the score card to ascertain their responsible AI innovation readiness level, and to determine the corresponding recommendations for moving forward on their journey towards responsible AI innovation. MY TOTAL SCORE Readiness LEVEL 1 LEVEL 2 LEVEL 3 LEVEL 4 LEVEL 5 Levels Culture: – 14 15 – 29 30 – 43 44 – 58 59 – 72 (Max Score 72) People and Expertise: – 17 18 – 34 35 – 51 52 – 68 69 – 84 (Max Score 84) Processes: – 14 15 – 29 30 – 43 44 – 58 59 – 72 (Max Score 72) TOTAL SCORE: 16

## Page 19

Organizational Readiness Assessment Questionnaire RECOMMENDATIONS FOR ORGANIZATIONAL READINESS Levels DESCRIPTION RECOMMENDATIONS At this level, your agency is just getting started toward responsible AI innovation. To improve your agency’s readiness, consider the following suggestions: • Conduct a thorough assessment of your agency’s ‘current capability to consider’ AI usage and identify any potential ethical or human rights risks. \|► Learn more about this in the Gaps & This stage of responsible AI Needs Analysis section of the innovation maturity typically sees Responsible AI Innovation in agencies with little or no experience Action Workbook. with or awareness of AI systems, • Develop an AI strategy that and they may or may not have some aligns with your agency’s values fundamental awareness of the and objectives. \|►Learn more potential ethical and human rights 1 about how to create an AI challenges. Generally, agencies at strategy from the annex of the this level will either not be using Organizational Roadmap. AI systems or will be beginning to consider using AI systems, although • Establish robust governance in some cases they may already be and oversight mechanisms using AI systems in furtherance of for AI development and their law enforcement objectives. deployment. • Cultivate a pool of AI talent and provide opportunities for officers and employees to upskill and develop their AI knowledge and expertise. • Explore opportunities to collaborate with industry partners, research institutions, and technology providers gain access to specialized expertise. 17

## Page 20

Organizational Readiness Assessment Questionnaire • Provide training and education to law enforcement agency personnel on general AI literacy and on AI ethics and human rights. • Encourage a culture that values ethical considerations, human rights, and accountability in AI-related endeavours and establish open channels for communication and feedback from law enforcement agency personnel. At this level, your agency still needs to develop more comprehensive and integrated policies and practices. To improve your readiness, consider the following suggestions: • Develop a comprehensive AI ethics policy that covers all This stage of responsible AI aspects of AI development and innovation maturity typically sees deployment. agencies actively exploring AI systems and their responsible • Integrate AI ethics into existing use. Agencies at this level will governance and oversight 2 be taking steps to conceptualize frameworks. their use of AI systems, including specific ethical and human rights • Continue to or start to guidance on selected AI use engage with the public and cases in a controlled environment multidisciplinary experts to (for instance, sandbox) before get feedback on your AI ethics rolling them out. This stage also policies and practices. may or may not already includes • Establish a process for engagement with the public and monitoring and evaluating multidisciplinary experts. the impact of your agency’s AI systems on ethical and human rights issues. \|► Learn more about this in the Use and Monitoring section of the Responsible AI Innovation in Action Workbook. 18

## Page 21

Organizational Readiness Assessment Questionnaire • Develop a training and education programme on responsible AI and human rights for all law enforcement personnel. • Integrate responsible AI principles into organizational policies and performance evaluations, reinforcing the importance of ethical considerations in decisionmaking processes and fostering a culture of continuous improvement. • Recognize and celebrate individuals and teams that embody responsible AI practices, fostering an environment where ethical behaviour is championed and rewarded. There is still room for improvement at this stage in terms of integrating ethics and human rights considerations into the design, development, and deployment of AI systems. To improve your This stage of responsible AI readiness, consider the following innovation maturity typically sees suggestions: agencies seeking to formalize • Strengthen the integration processes for responsible AI of ethics and human rights innovation. Agencies at this level considerations into the entire are, for instance, in the process of AI life cycle, from design to 3 rolling out AI systems following deployment and monitoring. successful public engagement and positive results from a pilot phase. \|► Learn more about this from Responsible AI Innovation in Agencies will have more structured Action Workbook. ethical and legal procedures and governance structures around the • Enhance stakeholder daily use of AI systems to support engagement and ensure the responsible use of communication to ensure these systems. transparency and accountability in AI decision-making processes. \|► Learn more about this by carrying out the Stakeholder Engagement Exercise in the Responsible AI Innovation in Action Workbook. 19

## Page 22

Organizational Readiness Assessment Questionnaire • Continuously evaluate and refine ethical and legal procedures to reflect evolving AI technologies and societal expectations and regulations to strengthen governance structures. • Foster a culture of ethical responsibility within the organization to encourage continuous improvement in responsible AI practices. • Provide leadership training and support to empower executives and managers to make and further foster a culture of responsible AI leadership within the agency. • Develop a strategy for talent acquisition and development tailored to the specific needs of AI initiatives within the agency. At this stage, it is important to focus on refining your agency’s practices to address the challenges arising from their expanding AI usage. To improve your readiness, This stage of responsible AI consider the following suggestions: innovation maturity typically sees agencies moving to more creative • Enhance AI development ways to use AI systems and, in processes to ensure fairness, turn, adjusting their responsible AI bias mitigation, and data innovation practices to address new governance is strictly followed. 4 challenges around development, stakeholder management, • Incorporate more evaluation, and monitoring. diverse perspectives and Agencies at this level will have interdisciplinary expertise into taken a significant leap in their use the assessment processes to of AI systems, from simple tasks ensure a holistic understanding to more complex tasks such as of risks and considerations. surveillance and facial recognition. • Implement rigorous evaluation and monitoring frameworks to assess AI system performance and impact to strengthen governance structures. 20

## Page 23

Organizational Readiness Assessment Questionnaire • Continuously adapt responsible AI practices to keep pace with evolving AI capabilities. • Strengthen stakeholder engagement strategies to incorporate diverse perspectives and address potential concerns. • Increase public awareness and understanding of AI use through targeted education and outreach initiatives. • Provide accessible and engaging educational resources to empower individuals to make informed decisions and participate in discussions about the responsible deployment of AI in law enforcement. • Promote collaboration with other law enforcement agencies, governmental bodies, and regulatory authorities to share best practices, exchange insights, and coordinate efforts in addressing ethical challenges related to AI usage. If your agency is at this stage, This stage of responsible AI it means it has a strong culture innovation maturity typically sees of responsible AI innovation. To a more widespread and routine improve your readiness, consider use of AI systems, with agencies the following suggestions: incorporating them into many of 5 their core functions, activities, • Continuously monitor and duties, roles, and processes. evaluate all AI systems for Agencies at this level will have potential ethical or human rights developed robust responsible risks. \|► Learn more about risk AI innovation practices and a gaps from the Risk Assessment sustainable culture across their Questionnaire. various units and departments. 21

## Page 24

Organizational Readiness Assessment Questionnaire • Foster a mindset of continuous improvement and innovation in responsible AI practices, encouraging experimentation, learning, and adaptation to emerging trends and challenges. • Support cross-functional teams and innovation initiatives focused on exploring novel approaches to ethical AI development and deployment, driving positive societal impact. • Invest in cutting-edge research and development initiatives to advance the frontiers of responsible AI innovation. • Establish platforms or forums for interagency collaboration to facilitate knowledge-sharing and collective problem-solving in the responsible use of AI systems. • Share responsible AI innovation practices and learnings with other agencies, departments, and units and encourage crosscollaboration. • Advocate for responsible AI innovation policies at the local, national, and international levels. • Engage with policymakers, regulatory bodies, and industry stakeholders to raise awareness of the importance of responsible AI practices and to influence policy development. 22

## Page 25

How to cite this publication: UNICRI and INTERPOL. (Revised February 2024). Toolkit for Responsible AI Innovation in Law Enforcement: Organizational Readiness Assessment Questionnaire. © United Nations Interregional Crime and Justice Research Institute (UNICRI), 2024 © International Criminal Police Organization (INTERPOL), 2024 2025 Update: This concerns a correction to minor errors contained in the previous version.

Images on this page:

\[image\]: media\Org_Readiness_Assessment_Mar25\page-25-img-1.png

## Page 26

www.ai-lawenforcement.org

Images on this page:

\[image\]: media\Org_Readiness_Assessment_Mar25\page-26-img-1.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-26-img-2.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-26-img-3.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-26-img-4.png

\[image\]: media\Org_Readiness_Assessment_Mar25\page-26-img-5.png
